{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 用 os.chdir() 跳到 main\n",
    "os.chdir('../main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"ppo-timeReward\"\n",
    "model_name = \"PPO\"\n",
    "checkpoints = range(500000, 7500000+1, 500000) # list of checkpoints to evaluate\n",
    "\n",
    "game_type = 3 # in [0, 1, 3] # [no_reset(1~12關), 一局, 三局兩勝]\n",
    "NUM_EXPERIMENTS = 100\n",
    "RENDERING = False    # Whether to render the game screen.\n",
    "RANDOM_ACTION = False\n",
    "\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    save_file = os.path.join(MODEL_DIR, MODEL_DIR+\"_\"+str(game_type)+\".csv\")\n",
    "\n",
    "\n",
    "# Model notes:\n",
    "# ppo_ryu_2000000_steps_updated: Just beginning to overfit state, generalizable but not quite capable.\n",
    "# ppo_ryu_2500000_steps_updated: Approaching the final overfitted state, cannot dominate first round but partially generalizable. High chance of beating the final stage.\n",
    "# ppo_ryu_3000000_steps_updated: Near the final overfitted state, almost dominate first round but barely generalizable.\n",
    "# ppo_ryu_7000000_steps_updated: Overfitted, dominates first round but not generalizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "if RENDERING:\n",
    "    virtual_display = Display(visible=0, size=(1400, 900))\n",
    "    virtual_display.start()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_500000_steps\n",
      "Average reward : -0.34672\n",
      "Winning rate: 0.0\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_1000000_steps\n",
      "Average reward : -0.31504000000000026\n",
      "Winning rate: 0.0\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_1500000_steps\n",
      "Average reward : -0.3256000000000002\n",
      "Winning rate: 0.0\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_2000000_steps\n",
      "Average reward : -0.27808000000000027\n",
      "Winning rate: 0.0\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_2500000_steps\n",
      "Average reward : -0.17248000000000008\n",
      "Winning rate: 0.05\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_3000000_steps\n",
      "Average reward : -0.13024000000000008\n",
      "Winning rate: 0.08\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_3500000_steps\n",
      "Average reward : 0.16719999999999996\n",
      "Winning rate: 0.26\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_4000000_steps\n",
      "Average reward : 0.24639999999999965\n",
      "Winning rate: 0.31\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_4500000_steps\n",
      "Average reward : 0.4734400000000001\n",
      "Winning rate: 0.53\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_5000000_steps\n",
      "Average reward : 0.46288000000000024\n",
      "Winning rate: 0.49\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_5500000_steps\n",
      "Average reward : 0.7233599999999996\n",
      "Winning rate: 0.7\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_6000000_steps\n",
      "Average reward : 0.7339199999999997\n",
      "Winning rate: 0.74\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_6500000_steps\n",
      "Average reward : 0.7163199999999996\n",
      "Winning rate: 0.71\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_7000000_steps\n",
      "Average reward : 0.6811199999999998\n",
      "Winning rate: 0.7\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_7500000_steps\n",
      "Average reward : 0.6547200000000004\n",
      "Winning rate: 0.66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 LIN Yi. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import time \n",
    "import csv\n",
    "\n",
    "import retro\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from street_fighter_custom_wrapper import StreetFighterCustomWrapper\n",
    "\n",
    "def make_env(game, state):\n",
    "    def _init():\n",
    "        env = retro.make(\n",
    "            game=game, \n",
    "            state=state, \n",
    "            use_restricted_actions=retro.Actions.FILTERED,\n",
    "            obs_type=retro.Observations.IMAGE\n",
    "        )\n",
    "        env = StreetFighterCustomWrapper(env, reset_round=game_type, rendering=RENDERING)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "game = \"StreetFighterIISpecialChampionEdition-Genesis\"\n",
    "env = make_env(game, state=\"Champion.Level12.RyuVsBison\")()\n",
    "\n",
    "if SAVE:\n",
    "    csvfile = open(save_file, 'w', newline='')\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"checkpoints\", \"average reward\", \"winning rate\"])\n",
    "\n",
    "for checkpts in checkpoints:\n",
    "    model_checkpoints = model_name + \"_\" + str(checkpts) + \"_steps\" # Speicify the model file to load. Model \"ppo_ryu_2500000_steps_updated\" is capable of beating the final stage (Bison) of the game.\n",
    "\n",
    "    if not RANDOM_ACTION:\n",
    "        model = PPO.load(os.path.join(MODEL_DIR, model_checkpoints), env=env)\n",
    "\n",
    "    num_experiments = NUM_EXPERIMENTS\n",
    "    experiment_reward_sum = 0\n",
    "    num_victory = 0\n",
    "\n",
    "    # print(\"\\nFighting Begins!\\n\")\n",
    "\n",
    "    for _ in range(1, num_experiments+1):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # print(\"\\nStart \", _, \"th experiment\")\n",
    "\n",
    "        if RENDERING:\n",
    "            img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "        while not done:\n",
    "            timestamp = time.time()\n",
    "\n",
    "            if RANDOM_ACTION:\n",
    "                obs, reward, done, info = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                action, _states = model.predict(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "\n",
    "            if reward != 0:\n",
    "                total_reward += reward\n",
    "                # print(\"Reward: {:.3f}, playerHP: {}, enemyHP:{}\".format(reward, info['agent_hp'], info['enemy_hp']))\n",
    "\n",
    "            if RENDERING:\n",
    "                img.set_data(env.render(mode='rgb_array'))\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "\n",
    "        if info['done_status'] == 1:\n",
    "            num_victory += 1\n",
    "            # print(_, \"th experiment Victory!\")\n",
    "        # else:\n",
    "            # print(_, \"th experiment Lose...\")\n",
    "\n",
    "        # print(\"Total reward: {}\\n\".format(total_reward))\n",
    "        experiment_reward_sum += total_reward\n",
    "\n",
    "    # print(\"\\nFighting Ends!\\n\")\n",
    "    if RANDOM_ACTION:\n",
    "        print(\"Random action\")\n",
    "    else:\n",
    "        print(model_checkpoints)\n",
    "\n",
    "    average_reward = experiment_reward_sum / num_experiments\n",
    "    winning_rate = 1.0 * num_victory / num_experiments\n",
    "\n",
    "    print(\"Average reward : {}\".format(average_reward))\n",
    "    print(\"Winning rate: {}\\n\".format(winning_rate))\n",
    "\n",
    "    if SAVE:\n",
    "        writer.writerow([checkpts, average_reward, winning_rate])\n",
    "\n",
    "# close file\n",
    "if SAVE:\n",
    "    csvfile.close()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if interrupt by KeyboardInterrupt, close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kol_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
