{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 用 os.chdir() 跳到 main\n",
    "os.chdir('../main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"ppo-timeReward\"\n",
    "model_name = \"PPO\"\n",
    "checkpoints = range(500000, 7500000+1, 500000) # list of checkpoints to evaluate\n",
    "\n",
    "game_type = 3 # in [0, 1, 3] # [no_reset(1~12關), 一局, 三局兩勝]\n",
    "NUM_EXPERIMENTS = 100\n",
    "RENDERING = False    # Whether to render the game screen.\n",
    "RANDOM_ACTION = False\n",
    "\n",
    "SAVE = True\n",
    "if SAVE:\n",
    "    save_file = os.path.join(MODEL_DIR, MODEL_DIR+\"_\"+str(game_type)+\".csv\")\n",
    "\n",
    "\n",
    "# Model notes:\n",
    "# ppo_ryu_2000000_steps_updated: Just beginning to overfit state, generalizable but not quite capable.\n",
    "# ppo_ryu_2500000_steps_updated: Approaching the final overfitted state, cannot dominate first round but partially generalizable. High chance of beating the final stage.\n",
    "# ppo_ryu_3000000_steps_updated: Near the final overfitted state, almost dominate first round but barely generalizable.\n",
    "# ppo_ryu_7000000_steps_updated: Overfitted, dominates first round but not generalizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "\n",
    "if RENDERING:\n",
    "    virtual_display = Display(visible=0, size=(1400, 900))\n",
    "    virtual_display.start()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_500000_steps\n",
      "Average reward : -0.35199999999999987\n",
      "Winning rate: 0.0\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_1000000_steps\n",
      "Average reward : -0.33792\n",
      "Winning rate: 0.01\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_1500000_steps\n",
      "Average reward : -0.32384000000000024\n",
      "Winning rate: 0.02\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_2000000_steps\n",
      "Average reward : -0.15488000000000013\n",
      "Winning rate: 0.14\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_2500000_steps\n",
      "Average reward : -0.04223999999999993\n",
      "Winning rate: 0.22\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_3000000_steps\n",
      "Average reward : -1.2212453270876722e-17\n",
      "Winning rate: 0.25\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_3500000_steps\n",
      "Average reward : 0.1830400000000001\n",
      "Winning rate: 0.38\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_4000000_steps\n",
      "Average reward : 0.19712000000000007\n",
      "Winning rate: 0.39\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_4500000_steps\n",
      "Average reward : 0.4787200000000002\n",
      "Winning rate: 0.59\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_5000000_steps\n",
      "Average reward : 0.6054399999999999\n",
      "Winning rate: 0.68\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_5500000_steps\n",
      "Average reward : 0.8588799999999988\n",
      "Winning rate: 0.86\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_6000000_steps\n",
      "Average reward : 0.7603199999999992\n",
      "Winning rate: 0.79\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_6500000_steps\n",
      "Average reward : 0.61952\n",
      "Winning rate: 0.69\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_7000000_steps\n",
      "Average reward : 0.7743999999999991\n",
      "Winning rate: 0.8\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "PPO_7500000_steps\n",
      "Average reward : 0.8447999999999988\n",
      "Winning rate: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2023 LIN Yi. All Rights Reserved.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import time \n",
    "import csv\n",
    "\n",
    "import retro\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from street_fighter_custom_wrapper import StreetFighterCustomWrapper\n",
    "\n",
    "def make_env(game, state):\n",
    "    def _init():\n",
    "        env = retro.make(\n",
    "            game=game, \n",
    "            state=state, \n",
    "            use_restricted_actions=retro.Actions.FILTERED,\n",
    "            obs_type=retro.Observations.IMAGE\n",
    "        )\n",
    "        env = StreetFighterCustomWrapper(env, reset_round=game_type, rendering=RENDERING)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "game = \"StreetFighterIISpecialChampionEdition-Genesis\"\n",
    "env = make_env(game, state=\"Champion.Level12.RyuVsBison\")()\n",
    "\n",
    "if SAVE:\n",
    "    csvfile = open(save_file, 'w', newline='')\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"model checkpoints\", \"average reward\", \"winning rate\"])\n",
    "\n",
    "for checkpts in checkpoints:\n",
    "    model_checkpoints = model_name + \"_\" + str(checkpts) + \"_steps\" # Speicify the model file to load. Model \"ppo_ryu_2500000_steps_updated\" is capable of beating the final stage (Bison) of the game.\n",
    "\n",
    "    if not RANDOM_ACTION:\n",
    "        model = PPO.load(os.path.join(MODEL_DIR, model_checkpoints), env=env)\n",
    "\n",
    "    num_experiments = NUM_EXPERIMENTS\n",
    "    experiment_reward_sum = 0\n",
    "    num_victory = 0\n",
    "\n",
    "    # print(\"\\nFighting Begins!\\n\")\n",
    "\n",
    "    for _ in range(1, num_experiments+1):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # print(\"\\nStart \", _, \"th experiment\")\n",
    "\n",
    "        if RENDERING:\n",
    "            img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "        while not done:\n",
    "            timestamp = time.time()\n",
    "\n",
    "            if RANDOM_ACTION:\n",
    "                obs, reward, done, info = env.step(env.action_space.sample())\n",
    "            else:\n",
    "                action, _states = model.predict(obs)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "\n",
    "            if reward != 0:\n",
    "                total_reward += reward\n",
    "                # print(\"Reward: {:.3f}, playerHP: {}, enemyHP:{}\".format(reward, info['agent_hp'], info['enemy_hp']))\n",
    "\n",
    "            if RENDERING:\n",
    "                img.set_data(env.render(mode='rgb_array'))\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "\n",
    "        if info['done_status'] == 1:\n",
    "            num_victory += 1\n",
    "            # print(_, \"th experiment Victory!\")\n",
    "        # else:\n",
    "            # print(_, \"th experiment Lose...\")\n",
    "\n",
    "        # print(\"Total reward: {}\\n\".format(total_reward))\n",
    "        experiment_reward_sum += total_reward\n",
    "\n",
    "    # print(\"\\nFighting Ends!\\n\")\n",
    "    if RANDOM_ACTION:\n",
    "        print(\"Random action\")\n",
    "    else:\n",
    "        print(model_checkpoints)\n",
    "\n",
    "    average_reward = experiment_reward_sum / num_experiments\n",
    "    winning_rate = 1.0 * num_victory / num_experiments\n",
    "\n",
    "    print(\"Average reward : {}\".format(average_reward))\n",
    "    print(\"Winning rate: {}\\n\".format(winning_rate))\n",
    "\n",
    "    if SAVE:\n",
    "        writer.writerow([model_checkpoints, average_reward, winning_rate])\n",
    "\n",
    "# close file\n",
    "if SAVE:\n",
    "    csvfile.close()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if interrupt by KeyboardInterrupt, close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kol_exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
